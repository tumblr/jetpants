#!/usr/bin/env ruby

# This is the jetpants command suite toolkit executable. It wraps the Jetpants
# module in a Thor command processor, providing a command-line interface to
# common Jetpants functionality.

%w[thor pry highline/import colored].each {|g| require g}

module Jetpants

  class CommandSuite < Thor

    def initialize *args
      super
      # setup pry
      Pry.config.prompt = proc {|object, nest_level, _| "# #{object} > "}
    end
    
    # Override Thor.dispatch to allow simple callbacks, which must be before_foo / 
    # after_foo *class* methods of Jetpants::CommandSuite. 
    # These aren't as full-featured as normal Jetpants::Callback: you can only have
    # ONE before_foo or after_foo method (they override instead of stacking); no arg
    # passing; no callback abort exception type. Mostly useful for plugins overriding
    # reminder text before or after a task.
    def self.dispatch(task, given_args, given_ops, config)
      task_name = task || given_args[0]
      self.send "before_#{task_name}" if self.respond_to? "before_#{task_name}"
      super
      self.send "after_#{task_name}" if self.respond_to? "after_#{task_name}"
    end
    
    
    desc 'console', 'Jetpants interactive console'
    def console
      Jetpants.pry
    end
    def self.before_console
      message = [ 'Welcome to the Jetpants Console.  A few notes:',
                  '  - Jetpants interacts with databases via ssh and the root user (BE CAREFUL).',
                  '  - Jetpants relies on having access to a root ssh key in order to perform these operations.',
                  '  - This console operates from inside the Jetpants module namespace by default (Jetpants::DB.new == DB.new, etc).',
                  '  - Jetpants uses a global config file at /etc/jetpants.yaml, or a user override config file at ~/.jetpants.yaml.',
                ].join "\n"
      print "\n#{message}\n\n"
    end
    
    
    desc 'promotion', 'perform a master promotion, changing which node is the master of a pool'
    method_option :demote,  :desc => 'node to demote'
    method_option :promote, :desc => 'node to promote'
    def promotion
      # It's not uncommon for the demoted master to be an offline/unavailable node, so relax Jetpants' normal
      # checks regarding replication threads being in different states.
      Jetpants.verify_replication = false
      
      promoted = options[:promote] ? options[:promote].to_db : nil
      demoted  = options[:demote]  ? options[:demote].to_db  : nil
      
      if promoted && !demoted
        error "Node to promote #{promoted} is not a slave" unless promoted.is_slave?
        demoted = promoted.master
        inform "Will demote #{demoted}, the master of specified promoted node #{promoted}."
      end
      
      if demoted
        demoted.probe
      else
        demoted = ask_node 'Please enter the IP address of the node to demote:'
      end

      if demoted.running?
        error 'Cannot demote a node that has no slaves!' unless demoted.has_slaves?
      else
        inform "Unable to connect to node #{demoted} to demote"
        error  "Unable to perform promotion" unless agree "Please confirm that #{demoted} is offline [yes/no]: "
        
        # An asset-tracker plugin may have been populated the slave list anyway
        if demoted.slaves && demoted.slaves.count > 0
          demoted.slaves.each {|s| s.probe}
        else
          replicas = ask("Please enter a comma-seperated list of IP addresses of all current replicas of #{demoted}: ").split /\s*,\s*/
          error "No replicas entered" unless replicas && replicas.count > 0
          error "User supplied list of replicas appears to be invalid - #{replicas}" unless replicas.all? {|replica| is_ip? replica}
          demoted.instance_eval {@slaves = replicas.map &:to_db}
          demoted.slaves.each do |replica|
            # Validate that they are really slaves of demoted
            error "#{replica} does not appear to be a valid replica of #{demoted}" unless replica.master == demoted
          end
        end
      end
      
      puts
      inform "Summary of affected pool"
      inform "Binary log positions and slave lag shown below are just a snapshot taken at the current time." if demoted.running?
      puts
      demoted.pool(true).summary(true)
      puts
      
      unless promoted
        if demoted.running?
          inform "Recommendation: promote the standby slave with the highest binary log coordinates"
        else
          inform "Recommendation: promote the standby slave or active slave with the highest binary log coordinates"
        end
        promoted = ask_node 'Please enter the IP address of the node to promote: '
      end
      
      error "Unable to determine a node to demote and a node to promote" unless demoted.kind_of?(Jetpants::DB) && promoted.kind_of?(Jetpants::DB)
      error "Node to promote #{promoted} is not a slave of node to demote #{demoted}" unless promoted.master == demoted
      error "The chosen node cannot be promoted. Please choose another." unless promoted.promotable_to_master?
      
      inform "Going to DEMOTE existing master #{demoted} and PROMOTE new master #{promoted}."
      error "Aborting." unless agree "Proceed? [yes/no]: "
      demoted.pool(true).master_promotion! promoted
    end
    def self.after_promotion
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
      )
    end
    
    
    desc 'summary', 'display information about a node in the context of its pool'
    method_option :node, :desc => 'IP address of node to query, or name of pool'
    def summary
      node = options[:node] || ask('Please enter node IP or name of pool: ')
      if is_ip? node
        node = node.to_db
        node.pool(true).probe
        describe node
        node.pool(true).summary(true)
      else
        pool = Jetpants.topology.pool(node)
        raise "#{node} is neither an IP address nor a pool name" unless pool
        pool.probe
        pool.summary(true)
      end
    end
    
    
    desc 'pools', 'display a full summary of every pool in the topology'
    def pools
      Jetpants.functional_partitions.concurrent_each &:probe
      Jetpants.shards.reject {|s| s.parent}.concurrent_each &:probe
      Jetpants.pools.each &:summary
      
      # We could do this more compactly using DB#role, but it would incorrectly
      # double-count nodes involved in a shard split
      counts = {master: 0, active_slave: 0, standby_slave: 0, backup_slave: 0}
      Jetpants.pools.each do |p|
        counts[:master] += 1
        counts[:active_slave] += p.active_slaves.count
        counts[:backup_slave] += p.backup_slaves.count
        counts[:standby_slave] += p.standby_slaves.count
      end
      
      puts
      puts "%4d global pools" % Jetpants.functional_partitions.count
      puts "%4d shard pools" % Jetpants.shards.count
      puts "---- --------------"
      puts "%4d total pools" % Jetpants.pools.count
      puts
      
      total = 0
      counts.each do |role, count|
        puts "%4d %ss" % [count, role.to_s.tr('_', ' ')]
        total += count
      end
      puts "---- --------------"
      puts "%4d total nodes" % total
      puts
    end

    desc 'pools_compact', 'display a compact summary (master, name, and size) of every pool in the topology'
    def pools_compact
      puts
      Jetpants.shards.each do |s| 
        puts "[%-15s] %8s to %-11s = %4s GB" % [s.ip, s.min_id, s.max_id, s.data_set_size(true)]
      end
      Jetpants.functional_partitions.each do |p| 
        puts "[%-15s] %-23s = %4s GB" % [p.ip, p.name, p.data_set_size(true)]
      end
      puts
    end
    
    
    desc 'regen_config', 'regenerate the application configuration'
    def regen_config
      Jetpants.topology.write_config
    end
    
    
    desc 'clone_slave', 'clone a standby slave'
    method_option :source, :desc => 'IP of node to clone from'
    method_option :target, :desc => 'IP of node(s) to clone to'
    def clone_slave
      puts "This task clones the data set of a standby slave."
      source = ask_node('Please enter IP of node to clone from: ', options[:source])
      source.master.probe if source.master # fail early if there are any replication issues in this pool
      describe source
      
      puts "You may clone to particular IP address(es), or can type \"spare\" to claim a node from the spare pool."
      target = options[:target] || ask('Please enter comma-separated list of targets (IPs or "spare") to clone to: ')
      target = 'spare' if target.strip == '' || target.split(',').length == 0
      spares_needed = target.split(',').count {|t| t.strip.upcase == 'SPARE'}
      if spares_needed > 0
        spares_available = Jetpants.topology.count_spares(role: :standby_slave, like: source)
        raise "Not enough spare machines with role of standby slave! Requested #{spares_needed} but only have #{spares_available} available." if spares_needed > spares_available
        claimed_spares = Jetpants.topology.claim_spares(spares_needed, role: :standby_slave, like: source)
      end

      targets = target.split(',').map do |ip|
        ip.strip!
        if is_ip? ip
          ip.to_db
        elsif ip == '' || ip.upcase == 'SPARE'
          claimed_spares.shift
        else
          error "target (#{ip}) does not appear to be an IP."
        end
      end
      
      source.start_mysql if ! source.running?
      error "source (#{source}) is not a standby slave" unless source.is_standby?
      
      targets.each do |t|
        error "target #{t} already has a master; please clear out node (including in asset tracker) before proceeding" if t.master
        error "target #{t} is running a different version of MySQL than source #{source}! Cannot proceed with clone operation." if t.version_cmp(source) != 0
      end
      
      source.enslave_siblings!(targets)
      targets.concurrent_each {|t| t.resume_replication; t.catch_up_to_master}
      source.pool.sync_configuration
      puts "Cloning complete."
      Jetpants.topology.write_config
    end
    
    
    desc 'activate_slave', 'turn a standby slave into an active slave'
    method_option :node, :desc => 'IP of standby slave to activate'
    def activate_slave
      puts "This task turns a standby slave into an active slave, OR alters an active slave's weight."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node

      weight = options[:weight] || ask('Please enter weight, or ENTER for default of 100: ')
      weight = 100 if weight == ''
      weight = weight.to_i
      error "Adding a slave of weight 0 makes no sense, use pull_slave instead" if weight == 0
      
      node.pool.mark_slave_active(node, weight)
      Jetpants.topology.write_config
    end
    
    
    desc 'weigh_slave', 'change the weight of an active slave'
    alias :weigh_slave :activate_slave
    
    
    desc 'pull_slave', 'turn an active slave into a standby slave'
    method_option :node, :desc => 'IP of active slave to pull'
    def pull_slave
      puts "This task turns an active slave into a standby slave."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      raise "Node is not an active slave" unless node.role == :active_slave
      node.pool.mark_slave_standby(node)
      Jetpants.topology.write_config
    end
    
    
    desc 'destroy_slave', 'remove a standby slave from its pool'
    method_option :node, :desc => 'IP of standby slave to remove'
    def destroy_slave
      # Permit slaves with broken replication to be destroyed
      Jetpants.verify_replication = false
      puts "This task removes a standby/backup slave from its pool entirely. THIS IS PERMANENT, ie, it does a RESET SLAVE on the target."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      if node.running? && node.available?
        raise "Node is not a standby or backup slave" unless (node.is_standby? || node.for_backups?)
      else
        puts "Please note that we cannot run a RESET SLAVE on the node, because MySQL is not running or is otherwise unreachable."
        puts "If the node is not permanently dead, you will have to run this manually."
      end
      raise "Aborting" unless ask('Please type YES in all capital letters to confirm removing node from its pool: ') == 'YES'
      node.pool.remove_slave!(node)
      node.revoke_all_access! if node.running? && node.available?
    end
    
    
    desc 'defrag_slave', 'export and re-import data set on a standby slave'
    method_option :node, :desc => 'IP of standby slave to defragment'
    def defrag_slave
      puts "This task exports all data on a standby/backup slave and then re-imports it."
      node = ask_node('Please enter node IP: ', options[:node])
      describe node
      raise "Node is not a standby or backup slave" unless (node.is_standby? || node.for_backups?)
      raise "Cannot defragment non-shard slaves from command suite yet; use jetpants console instead" unless node.pool.is_a?(Shard)
      node.rebuild!
    end
    
    
    desc 'shard_read_only', 'mark a shard as read-only'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as read-only'
    def shard_read_only
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :read_only
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_offline', 'mark a shard as offline (not readable or writable)'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as offline'
    def shard_offline
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :offline
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_online', 'mark a shard as fully online (readable and writable)'
    method_option :min_id, :desc => 'Minimum ID of shard to mark as fully online'
    def shard_online
      shard_min = options[:min_id] || ask('Please enter min ID of the shard: ')
      s = Jetpants.topology.shard shard_min
      raise "Shard not found" unless s
      s.state = :ready
      s.sync_configuration
      Jetpants.topology.write_config
    end
    
    
    desc 'shard_split', 'shard split step 1 of 4: spin up child pools with different portions of data set'
    method_option :min_id, :desc => 'Minimum ID of parent shard to split'
    method_option :max_id, :desc => 'Maximum ID of parent shard to split'
    method_option :ranges, :desc => 'Optional comma-separated list of ranges per child ie "1000-1999,2000-2499" (default if omitted: split evenly)'
    method_option :count,  :desc => 'How many child shards to split the parent into (only necessary if the ranges option is omitted)'
    def shard_split
      shard_min = options[:min_id] || ask('Please enter min ID of the parent shard: ')
      shard_max = options[:max_id] || ask('Please enter max ID of the parent shard: ')
      s = Jetpants.topology.shard shard_min, shard_max
      
      raise "Shard not found" unless s
      raise "Shard isn't in ready state" unless s.state == :ready
      raise "Cannot split the last shard (max ID of infinity), use \"jetpants shard_cutover\" instead" if s.max_id == 'INFINITY'
      
      ranges = options[:ranges] || ask('Optionally enter comma-separated list of ranges per child ie "1000-1999,2000-2499" [default=even split]: ')
      ranges = false if ranges.strip == ''
      
      if ranges
        supply_ranges = []
        ranges.split(',').each do |my_range|
          my_range.strip!
          my_min, my_max = my_range.split('-', 2)
          my_min = my_min.strip.to_i
          my_max = my_max.strip.to_i
          raise "Supplied range list has gaps!" if supply_ranges.last && supply_ranges.last[1] + 1 != my_min
          supply_ranges << [my_min, my_max]
        end
        children = supply_ranges.count
        raise "Supplied range does not cover parent completely!" if supply_ranges.first[0] != shard_min.to_i || supply_ranges.last[1] != shard_max.to_i
        s.split!(children, supply_ranges)
      else
        children = options[:count] || ask('Optionally enter how many children to split into [default=2]: ')
        children = 2 if children == ''
        s.split!(children.to_i)
      end
    end
    def self.before_shard_split
      reminders(
        'This process may take several hours. You probably want to run this from a screen session.',
        'Be especially careful if you are relying on SSH Agent Forwarding for your root key, since this is not screen-friendly.'
      )
    end
    def self.after_shard_split
      reminders(
        'Proceed to next step: jetpants shard_split_child_reads'
      )
    end
    
    
    # This step is only really necessary if asset-tracker changes don't immediately reflect in application configuration.
    # (ie, if app configuration is a static file that needs to be deployed to webs.)
    desc 'shard_split_child_reads', 'shard split step 2 of 4: move reads to child shards'
    def shard_split_child_reads
      Jetpants.topology.write_config
    end
    def self.after_shard_split_child_reads
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for reads to stop on the old parent master.',
        'Proceed to next step: jetpants shard_split_child_writes'
      )
    end
    
    
    desc 'shard_split_child_writes', 'shard split step 3 of 4: move writes to child shards'
    method_option :min_id, :desc => 'Minimum ID of parent shard being split'
    method_option :max_id, :desc => 'Maximum ID of parent shard being split'
    def shard_split_child_writes
      s = ask_shard_being_split
      s.move_writes_to_children
      Jetpants.topology.write_config
    end
    def self.after_shard_split_child_writes
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
        'Wait for writes to stop on the old parent master.',
        'Proceed to next step: jetpants shard_split_cleanup',
      )
    end
    
    
    desc 'shard_split_cleanup', 'shard split step 4 of 4: clean up data that replicated to wrong shard'
    method_option :min_id, :desc => 'Minimum ID of parent shard being split'
    method_option :max_id, :desc => 'Maximum ID of parent shard being split'
    def shard_split_cleanup
      s = ask_shard_being_split
      s.cleanup!
    end
    def self.after_shard_split_cleanup
      reminders(
        'Review old nodes for hardware issues before re-using, or simply cancel them.',
      )
    end
    
    
    desc 'shard_cutover', 'truncate the current last shard range, and add a new shard after it'
    method_option :cutover_id, :desc => 'Minimum ID of new last shard being created'
    def shard_cutover
      cutover_id = options[:cutover_id] || ask('Please enter min ID of the new shard to be created: ')
      cutover_id = cutover_id.to_i
      last_shard = Jetpants.topology.shards.select {|s| s.max_id == 'INFINITY' && s.in_config?}.first
      last_shard_master = last_shard.master
      
      # Simple sanity-check that the cutover ID is greater than the current last shard's MIN id.
      # (in a later release, this may be improved to also ensure it's greater than any sharding
      # key value in use on the last shard.)
      raise "Specified cutover ID is too low!" unless cutover_id > last_shard.min_id
      
      # Ensure enough spare nodes are available before beginning.
      # We supply the *previous* last shard as context for counting spares
      # because the new last shard can't be created yet (chicken-and-egg problem -- master
      # must exist before we create the pool). The assumption is the hardware spec
      # of the new last shard and previous last shard will be the same.
      raise "Not enough total spare machines!" unless Jetpants.topology.count_spares(like: last_shard_master) >= Jetpants.standby_slaves_per_pool + 1
      raise "Not enough standby_slave role spare machines!" unless Jetpants.topology.count_spares(role: :standby_slave, like: last_shard_master) >= Jetpants.standby_slaves_per_pool
      raise "Cannot find a spare master-role machine!" unless Jetpants.topology.count_spares(role: :master, like: last_shard_master) >= 1
      
      # In asset tracker, remove the last shard pool and replace it with a new pool. The new pool
      # has the same master/slaves but now has a non-infinity max ID.
      last_shard.state = :recycle
      last_shard.sync_configuration
      last_shard_replace = Shard.new(last_shard.min_id, cutover_id - 1, last_shard_master)
      last_shard_replace.sync_configuration
      Jetpants.topology.pools << last_shard_replace
      
      # Now put another new shard after that one. (See earlier comment as to why we're supplying
      # the pool from the old last shard. This param is just used to select the right type of hardware,
      # NOT to actually set the pool of the returned object.)
      new_last_shard_master = Jetpants.topology.claim_spare(role: :master, like: last_shard_master)
      new_last_shard_master.disable_read_only! if new_last_shard_master.running?
      if Jetpants.standby_slaves_per_pool > 0
        # Verify spare count again, now that we can actually supply the new master as the :like context
        raise "Not enough standby_slave role spare machines!" unless Jetpants.topology.count_spares(role: :standby_slave, like: new_last_shard_master) >= Jetpants.standby_slaves_per_pool
        new_last_shard_slaves = Jetpants.topology.claim_spares(Jetpants.standby_slaves_per_pool, role: :standby_slave, like: new_last_shard_master)
        new_last_shard_slaves.each do |x| 
          x.change_master_to new_last_shard_master
          x.resume_replication
        end
      end
      new_last_shard = Shard.new(cutover_id, 'INFINITY', new_last_shard_master)
      new_last_shard.sync_configuration
      Jetpants.topology.pools << new_last_shard
      
      # Create tables on the new shard's master, obtaining schema from previous last shard
      tables = Table.from_config 'sharded_tables'
      last_shard_master.export_schemata tables
      last_shard_master.host.fast_copy_chain(Jetpants.export_location, new_last_shard_master.host, files: ["create_tables_#{last_shard_master.port}.sql"])
      new_last_shard_master.import_schemata!
      
      # regen config file
      Jetpants.topology.write_config
    end
    def self.after_shard_cutover
      reminders(
        'Commit/push the configuration in version control.',
        'Deploy the configuration to all machines.',
      )
    end
    
    
    no_tasks do
      def is_ip? address
        address =~ /(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})/
      end

      def error message
        abort ['ERROR:'.red, message].join ' '
      end

      def inform message
        puts message.blue
      end
      
      def describe node
        puts "Node #{node} (#{node.hostname}:#{node.port}) has role #{node.role} in pool #{node.pool(true)}.".green
      end
      
      def ask_node(prompt, supplied_node=false)
        node = supplied_node || ask(prompt)
        error "Node (#{node}) does not appear to be an IP address." unless is_ip? node
        node.to_db
      end
      
      def ask_shard_being_split
        shards_being_split = Jetpants.shards.select {|s| s.children.count > 0}
        if shards_being_split.count == 0
          raise 'No shards are currently being split. You can only use this task after running "jetpants shard_split".'
        elsif shards_being_split.count == 1
          s = shards_being_split[0]
          puts "Detected #{s} as the only shard currently involved in a split operation."
          error "Aborting." unless agree "Is this the right shard that you want to perform this action on? [yes/no]: "
        else
          puts "The following shards are currently involved in a split operation:"
          shards_being_split.each {|sbs| puts "* #{sbs}"}
          puts "Which shard would you like to perform this action on?"
          shard_min = options[:min_id] || ask('Please enter min ID of the parent shard: ')
          shard_max = options[:max_id] || ask('Please enter max ID of the parent shard: ')
          s = Jetpants.topology.shard shard_min, shard_max
          raise "Shard not found" unless s
        end
        raise "Shard isn't in expected state" unless s.state == :deprecated
        s
      end
    end
    
    def self.reminders(*strings)
      strings.map! {|s| "  - #{s}"}
      strings.flatten!
      reminder_text = strings.join "\n"
      noun = (strings.count == 1 ? 'REMINDER' : 'REMINDERS')
      print "\n#{noun}:\n#{reminder_text}\n\n" if strings.count > 0
    end
  end
end

# We load jetpants last so that plugins can monkeypatch Jetpants::CommandSuite if desired.
require 'jetpants'

Jetpants::CommandSuite.start
